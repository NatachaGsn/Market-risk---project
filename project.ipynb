{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02edc8a9",
   "metadata": {},
   "source": [
    "# Market Risk project\n",
    "\n",
    "FOURREAU Mathis\n",
    "\n",
    "GAUSSIN Natacha\n",
    "\n",
    "ESILV IF3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db52e3",
   "metadata": {},
   "source": [
    "# Library and dataset importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff28d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.integrate import cumulative_trapezoid as cumtrapz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f68b3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>5.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>5.424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-06</td>\n",
       "      <td>5.329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-07</td>\n",
       "      <td>5.224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-08</td>\n",
       "      <td>5.453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>2018-12-21</td>\n",
       "      <td>4.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>4.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>3.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>4.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>4.119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1023 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  value\n",
       "0    2015-01-02  5.621\n",
       "1    2015-01-05  5.424\n",
       "2    2015-01-06  5.329\n",
       "3    2015-01-07  5.224\n",
       "4    2015-01-08  5.453\n",
       "...         ...    ...\n",
       "1018 2018-12-21  4.045\n",
       "1019 2018-12-24  4.010\n",
       "1020 2018-12-27  3.938\n",
       "1021 2018-12-28  4.088\n",
       "1022 2018-12-31  4.119\n",
       "\n",
       "[1023 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"Natixis.csv\", sep = \";\")\n",
    "\n",
    "# Transform the date column to datetime and sort the dataframe by date\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%d/%m/%Y\")\n",
    "df.sort_values(\"date\", inplace = True)\n",
    "\n",
    "# Transform the value column to numeric\n",
    "df[\"value\"] = (df[\"value\"].astype(str).str.replace(\",\", \".\", regex=False))\n",
    "df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "\n",
    "# display the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793a1cd8",
   "metadata": {},
   "source": [
    "## Question A (Ex2, part of Q1 and of Q2 of TD1)\n",
    "\n",
    "**a** – From the time series of the daily prices of the stock Natixis between January 2015 and December 2016, provided with TD1, estimate a historical VaR on price returns at a one-day horizon for a given probability level (this probability is a parameter which must be changed easily). You must base your VaR on a non-parametric distribution (biweight Kernel, that is $K$ is the derivative of the logistic function $x \\mapsto \\frac{15}{16}(1-x^2)^2 \\mathbb{1}_{|x| \\leq 1}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75574fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the returns\n",
    "df[\"return\"] = df[\"value\"] / df[\"value\"].shift(1) - 1\n",
    "\n",
    "# QUESTION : EST CE QUE LES RETURNS SONT BONS ICI CAR LES VARIATIONS DE TEMPS NE SONT PAS CONSTANTES ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45bb76f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     -0.035047\n",
       "2     -0.017515\n",
       "3     -0.019704\n",
       "4      0.043836\n",
       "5     -0.020723\n",
       "         ...   \n",
       "508   -0.008118\n",
       "509    0.000744\n",
       "510   -0.000186\n",
       "511   -0.009481\n",
       "512    0.006006\n",
       "Name: return, Length: 512, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract returns for the years prior to 2017 (2015 and 2016)\n",
    "df_2015_2016 = df[df[\"date\"] < \"2017\"].dropna().loc[:, \"return\"]\n",
    "df_2015_2016"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1763e8f9",
   "metadata": {},
   "source": [
    "Ce qu'on a fait (tout à refaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bac20c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The empirical VaR is for returns between 2015 and 2016 is :  -6.007630146624207 %\n",
      "There are -6.01 % chances to lose 1.0 % of the portfolio's value\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.99\n",
    "\n",
    "def Kde_VaR(df, alpha, bw=None):\n",
    "    # Fit KDE (Gaussian kernel)\n",
    "    kde = gaussian_kde(df, bw_method=bw)  # 'scott' by default\n",
    "\n",
    "    # Build a grid covering the tail well\n",
    "    mu, s = np.mean(df), np.std(df, ddof=1)\n",
    "    lo = min(df.min(), mu - 6*s)\n",
    "    hi = max(df.max(), mu + 6*s)\n",
    "    grid = np.linspace(lo, hi, 20001)\n",
    "\n",
    "    # PDF on grid and CDF by numerical integration (trapezoid)\n",
    "    pdf = kde(grid)\n",
    "    cdf = cumtrapz(pdf, grid, initial=0.0)\n",
    "    cdf = cdf / cdf[-1]  # normalize to 1\n",
    "\n",
    "    # α-quantile by interpolation of the CDF\n",
    "    q_alpha = np.interp(alpha, cdf, grid)\n",
    "\n",
    "    return -q_alpha\n",
    "\n",
    "kde_var = Kde_VaR(df_2015_2016, alpha)\n",
    "\n",
    "print(\"The empirical VaR is for returns between 2015 and 2016 is : \", kde_var*100, \"%\")\n",
    "print(\"There are\", round(kde_var*100, 2), \"% chances to lose\", round((1 - alpha)*100, 2), \"% of the portfolio's value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021fbaba",
   "metadata": {},
   "source": [
    "### First step: Estimation of the kernel density \n",
    "$$\\hat{f}(x) = \\frac{1}{nh}\\sum_{i=1}^{n} K\\left(\\frac{x - X_i}{h}\\right)$$ with $$K(x) = \\frac{15}{16}(1-x^2)^2 \\mathbb{1}_{|x| \\leq 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e7a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def K(x)\n",
    "def K(x):\n",
    "    K_tab = []\n",
    "    for i in x:\n",
    "        if i <= 1 and i >= -1:\n",
    "            K_tab.append((15/16) * (1 - i**2)**2)\n",
    "        else:\n",
    "            K_tab.append(0)\n",
    "    return K_tab\n",
    "\n",
    "# def f_hat(x, h, tab_returns)\n",
    "def f_hat(x, h, tab_returns):\n",
    "    return sum(K((x - tab_returns) / h)) / (len(tab_returns) * h)\n",
    "\n",
    "# estimate 1000 times f_hat\n",
    "def estimate_f_hat(tab_returns, h, nb_estimations):\n",
    "    x_tab = np.linspace(min(tab_returns) - 5*h, max(tab_returns) + 5*h, nb_estimations)\n",
    "    f_hat_tab = []\n",
    "    for x in x_tab:\n",
    "        f_hat_tab.append(f_hat(x, h, tab_returns))\n",
    "\n",
    "    return f_hat_tab, x_tab\n",
    "\n",
    "def ComputeVaR(tab_returns, h, alpha, nb_estimations = 1000):\n",
    "    f_hat_tab, x_tab = estimate_f_hat(tab_returns, h, nb_estimations)\n",
    "    F_hat = 0\n",
    "    i = -1\n",
    "    delta = x_tab[1] - x_tab[0]\n",
    "    while F_hat < alpha and i < len(f_hat_tab) - 1:\n",
    "        i+=1\n",
    "        F_hat += f_hat_tab[i] * delta\n",
    "\n",
    "    return x_tab[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0404add3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "448655ab",
   "metadata": {},
   "source": [
    "### Second Step : Choice of h\n",
    "\n",
    "- Tracer Graph de la densité pour different h (voir comment la densité évolue), avoir un graphique du style de p72 dans le cours.\n",
    "- Utiliser un résultat théorique pour le h optimal (papier de recherche etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf9e0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c221affa",
   "metadata": {},
   "source": [
    "### 3rd Step : Define the CDF of K and use\n",
    "\n",
    "$$\\hat{F}(x) = \\frac{1}{n}\\sum_{i=1}^{n} \\mathcal{K}\\left(\\frac{x - X_i}{h}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f28c8ab",
   "metadata": {},
   "source": [
    "To define $\\mathcal{K}$ we have to primitive K.\n",
    "\n",
    "$$\n",
    "\\mathcal{K}(u) = \\begin{cases}\n",
    "0 & \\text{si } u < -1 \\\\[0.5em]\n",
    "\\frac{1}{2} + \\frac{15}{16}\\left(u - \\frac{2u^3}{3} + \\frac{u^5}{5}\\right) & \\text{si } -1 \\leq u \\leq 1 \\\\[0.5em]\n",
    "1 & \\text{si } u > 1\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0231252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05750841439123988\n"
     ]
    }
   ],
   "source": [
    "#def K_cdf(x):\n",
    "def K_cdf(x):\n",
    "    if x <= 1 and x >= -1:\n",
    "        return (15/16)*(x - 2*x**3/3 + x**5/5 + 8/15)\n",
    "    elif x < -1:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "def K_cdf_vector(u_tab):\n",
    "    return [K_cdf(u) for u in u_tab]\n",
    "\n",
    "def F_hat(x, h, returns_tab):\n",
    "    u_tab = (x-returns_tab)/h\n",
    "    return sum(K_cdf_vector(u_tab)) / len(returns_tab)\n",
    "\n",
    "def Series_F_Hat(h, returns_tab, x_tab):\n",
    "    return [F_hat(x, h, returns_tab) for x in x_tab]\n",
    "\n",
    "def ComputeVaR(h, returns_tab, alpha, nb_estimations=1000):\n",
    "    x_tab = np.linspace(min(returns_tab) - 5*h, max(returns_tab) + 5*h, nb_estimations)\n",
    "    F_hat_tab = Series_F_Hat(h, returns_tab, x_tab)\n",
    "    i = 0\n",
    "    while i < len(F_hat_tab):\n",
    "        if F_hat_tab[i] >= alpha:\n",
    "            return x_tab[i]\n",
    "        i+=1\n",
    "\n",
    "    return x_tab[len(x_tab) - 1]\n",
    "\n",
    "print(ComputeVaR(0.003, df_2015_2016, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eb2d00",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8fb3b61",
   "metadata": {},
   "source": [
    "### Step 4: Find the VaR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction du style:\n",
    "\n",
    "\n",
    "def kernel_VaR(X, h, alpha, number_of_points):\n",
    "    # 1. Calculer F_hat(x) pour tous les points x\n",
    "    x, y = cumulative_kernel_density(X, h, number_of_points)\n",
    "    # x = grille de valeurs (1000 points entre min et max des rendements)\n",
    "    # y = F_hat(x) pour chaque point (valeurs de la CDF)\n",
    "    \n",
    "    # 2. Trouver l'index où y >= alpha pour la première fois\n",
    "    y_VaR = np.argmax(y >= alpha)\n",
    "    # argmax retourne l'INDEX du premier True dans le tableau booléen\n",
    "    # Exemple: si alpha=0.05, on cherche le premier point où F_hat >= 0.05\n",
    "    \n",
    "    # 3. Retourner la valeur de x correspondant à cet index\n",
    "    return x[y_VaR]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d669b10f",
   "metadata": {},
   "source": [
    "**b – Which proportion of price returns between January 2017 and December 2018 does exceed the VaR\n",
    "threshold defined in the previous question? Do you validate the choice of this non-parametric VaR?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0951e7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513     0.007463\n",
       "514     0.040741\n",
       "515     0.003737\n",
       "516    -0.008155\n",
       "517    -0.005719\n",
       "          ...   \n",
       "1018   -0.001481\n",
       "1019   -0.008653\n",
       "1020   -0.017955\n",
       "1021    0.038090\n",
       "1022    0.007583\n",
       "Name: return, Length: 510, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2017_2018 = df[df[\"date\"] >= \"2017\"].dropna().loc[:, \"return\"]\n",
    "df_2017_2018"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d22515",
   "metadata": {},
   "source": [
    "Simply find the proportion of returns < to the VaR find and compare with the alpha. Mentionne the word coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52833dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#j'ai simplifié la fonction qu'on avait dans notr elab car une seul VaR\n",
    "def Proportion(df, VaR):\n",
    "    prop = df[df < VaR].count() / df.count()\n",
    "    return prop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca64d6e3",
   "metadata": {},
   "source": [
    "**Question B (Ex2, Q5 of TD2)\n",
    "Calculate the expected shortfall for the VaR calculated in question A. How is the result, compared to\n",
    "the VaR?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_beyond_VaR = df_2015_2016[df_2015_2016[\"returns\"] <= VaR_alpha]\n",
    "ES_alpha = np.mean(losses_beyond_VaR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d153ea",
   "metadata": {},
   "source": [
    "**Question C (Ex2, Q1 and Q2 of TD3)\n",
    "\n",
    "With the dataset provided for TD1 on Natixis prices, first calculate daily returns. You will then analyse\n",
    "these returns using a specific method in the field of the EVT.\n",
    "\n",
    "a – Estimate the GEV parameters for the two tails of the distribution of returns, using the estimator of\n",
    "Pickands. What can you conclude about the nature of the extreme gains and losses?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "830f98b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_loss : -0.5089715779341932\n",
      "e_gain : -0.9367427921518837\n"
     ]
    }
   ],
   "source": [
    "# Ce qu'on a fait TD3\n",
    "\n",
    "# loss\n",
    "loss = list(df[df[\"return\"] < 0][\"return\"] * -1)\n",
    "loss.sort()\n",
    "n = len(loss)\n",
    "e_loss = np.log((loss[int(n-1 - np.floor(np.log(n)) + 1)] - loss[int(n-1 - 2 * np.floor(np.log(n)) + 1)]) / (loss[int(n-1 - 2 * np.floor(np.log(n)) + 1)] - loss[int(n-1 - 4 * np.floor(np.log(n)) + 1)]))\n",
    "e_loss /= np.log(2)\n",
    "print(\"e_loss :\", e_loss)\n",
    "\n",
    "# gain\n",
    "gain = list(df[df[\"return\"] > 0][\"return\"] * -1)\n",
    "gain.sort()\n",
    "n = len(gain)\n",
    "e_gain = np.log((gain[int(n-1 - np.floor(np.log(n)) + 1)] - gain[int(n-1 - 2 * np.floor(np.log(n)) + 1)]) / (gain[int(n-1 - 2 * np.floor(np.log(n)) + 1)] - gain[int(n-1 - 4 * np.floor(np.log(n)) + 1)]))\n",
    "e_gain /= np.log(2)\n",
    "print(\"e_gain :\", e_gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e066da",
   "metadata": {},
   "source": [
    "**b – Calculate the value at risk based on EVT for various confidence levels, with the assumption of iid\n",
    "returns.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac285a18",
   "metadata": {},
   "source": [
    "**Formule du cours**\n",
    "\n",
    "$$VaR(p) = \\frac{\\left(\\frac{k}{n(1-p)}\\right)^{\\xi^P} - 1}{1 - 2^{-\\xi^P}} \\left(X_{n-k+1:n} - X_{n-2k+1:n}\\right) + X_{n-k+1:n}$$\n",
    "\n",
    "p is the probability to be above the VaR (so alpha = 0.99 -> p = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b79ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74753208",
   "metadata": {},
   "source": [
    "Il faudra bien voir si la VaR trouvé reste cohérente pour voir qi nos Epsilon estimés le sont aussi. C'est le epsilon loss qui nous intéresse dans cette partie. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d009828",
   "metadata": {},
   "source": [
    "**Question D (Ex2, Q5 of TD4)**\n",
    "\n",
    "With the dataset and the framework provided for TD4, estimate all the parameters of Bouchaud's price\n",
    "impact model. Comment the obtained values. Is this model well specified?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee5f2d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b8c2f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TD4 = pd.read_excel(\"Dataset TD4.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30069f",
   "metadata": {},
   "source": [
    "**Backward definition of the price: the price is the sum of past impacts (Bouchaud):**\n",
    "\n",
    "$$p_t = p_{-\\infty} + \\sum_{s=-\\infty}^{t-1} G(t-s)\\varepsilon_s S_s V_s^r$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $S$ is the bid-ask spread;\n",
    "\n",
    "- $\\varepsilon$ is -1 or 1 depending on whether the transaction is a buy (price = ask) or a sell (price = bid) on the market;\n",
    "\n",
    "- $V$ is the volume of the transaction, with $r$ close to zero to have a concave function of the volume;\n",
    "\n",
    "- $G$ is a function worth 0 on $\\mathbb{R}^-$, which can be interpreted as the impact of a single order. A statistical study on long-term correlation can allow this function to be fixed, in the form, for example, of a decreasing power function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd2669f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
